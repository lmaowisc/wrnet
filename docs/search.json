[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "",
    "text": "WRNet is a machine-learning approach to regularized win ratio regression for hierarchical composite endpoints. In the presence of many features, it optimizes feature selection and risk prediction through an elastic net-type penalty on the regression coefficients (log-win ratios)."
  },
  {
    "objectID": "index.html#win-ratio-basics",
    "href": "index.html#win-ratio-basics",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "Win ratio basics",
    "text": "Win ratio basics"
  },
  {
    "objectID": "index.html#regularized-win-ratio-regression",
    "href": "index.html#regularized-win-ratio-regression",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "Regularized win ratio regression",
    "text": "Regularized win ratio regression"
  },
  {
    "objectID": "index.html#basics",
    "href": "index.html#basics",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "Basics",
    "text": "Basics\nComparing two subjects at time \\(t\\), a winner is the one that has either\n\nlonger overall survival, or\nlonger event-free survival time if both survive past \\(t\\).\n\nA basic win ratio model is \\[\\begin{equation}\n\\frac{P(\\mbox{Subject $i$ wins by time $t$})}{P(\\mbox{Subject $j$ wins by time $t$})}\n=\\exp\\{\\beta^{\\rm T}(z_i - z_j)\\}.\n\\end{equation}\\]\nIn the case with a high-dimensional \\(z\\), a mixture of penalties on the \\(L_1\\) (lasso) and \\(L_2\\) (ridge regression) norms of \\(\\beta\\) is imposed to regularize the model."
  },
  {
    "objectID": "index.html#a-step-by-step-example",
    "href": "index.html#a-step-by-step-example",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "A step-by-step example",
    "text": "A step-by-step example\n\nInstallation\nCompile the R functions defined in https://github.com/lmaowisc/wrnet/wrnet_functions.R.\n\nsource(\"wrnet_functions.R\")\n\nTwo packages that are used extensively by these functions are glmnet and tidyverse.\n\nlibrary(glmnet) # for elastic net\nlibrary(tidyverse) # for data manipulation/visualization\n\n\n\nData preparation\nConsider\n\n\nTuning parameter selection\n\n\nFinal model and evaluation"
  },
  {
    "objectID": "index.html#a-step-by-step-guide",
    "href": "index.html#a-step-by-step-guide",
    "title": "wrnet: Regularized win ratio regression through elastic net",
    "section": "A step-by-step guide",
    "text": "A step-by-step guide\n\nInstallation\nDownload and compile R functions in wrnet_functions.R (available at https://github.com/lmaowisc/wrnet).\n\nsource(\"wrnet_functions.R\")\n\nTwo packages used extensively in these functions are glmnet and tidyverse.\n\nlibrary(glmnet) # for elastic net\nlibrary(tidyverse) # for data manipulation/visualization\n\n\n\nData preparation\nConsider a German breast cancer study with 686 subjects and 9 covariates.\n\n# Load package containing data\nlibrary(WR)\n# Load data\ndata(\"gbc\") \ndf &lt;- gbc # n = 686 subjects, p = 9 covariates\ndf # status = 0 (censored), 1 (death), 2 (recurrence)\n#&gt;   id      time status hormone age menopause size grade ...\n#&gt;1   1 43.836066      2       1  38         1   18     3  \n#&gt;2   1 74.819672      0       1  38         1   18     3  \n#&gt;3   2 46.557377      2       1  52         1   20     1   \n#&gt;4   2 65.770492      0       1  52         1   20     1  \n#&gt;5   3 41.934426      2       1  47         1   30     2   \n#&gt;...\n\nSplit data into training versus test set:\n\n# Data partitioning ------------------------------------\nset.seed(123)\nobj_split &lt;- df |&gt; wr_split() # default: 80% training, 20% test\n# Take training and test set\ndf_train &lt;- obj_split$df_train\ndf_test &lt;- obj_split$df_test\n\n\n\nTuning-parameter selection\nPerform 10-fold (default) cross-validation on training set:\n\n# 10-fold CV -------------------------------------------\nset.seed(1234)\nobj_cv &lt;- cv_wrnet(df_train$id, df_train$time, df_train$status, \n                    df_train |&gt; select(-c(id, time, status)))\n# Plot CV results (C-index vs log-lambda)\nobj_cv |&gt; \n   ggplot(\n    aes(x =  log(lambda), y = concordance)\n   ) +\n   geom_point() +\n   geom_line() +\n    theme_minimal()\n\n\n\n\n\n\n\n# Optimal lambda\nlambda_opt &lt;- obj_cv$lambda[which.max(obj_cv$concordance)]\nlambda_opt\n#&gt; [1] 0.0171976\n\n\n\nFinal model and evaluation\nFinalize model at optimal tuning parameter \\(\\lambda_{\\rm opt}\\):\n\n# Final model ------------------------------------------\nfinal_fit &lt;- wrnet(df_train$id, df_train$time, df_train$status, \n              df_train |&gt; select(-c(id, time, status)), \n              lambda = lambda_opt)\n# Estimated coefficients\nfinal_fit$beta\n#&gt; 8 x 1 sparse Matrix of class \"dgCMatrix\"\n#&gt;                      s0\n#&gt; hormone     0.306026364\n#&gt; age         0.003111462\n#&gt; menopause   .          \n#&gt; size       -0.007720497\n#&gt; grade      -0.285511701\n#&gt; nodes      -0.082227827\n#&gt; prog_recp   0.001861367\n#&gt; estrg_recp  .     \n# Variable importance plot\nfinal_fit |&gt; \n   vi_wrnet() |&gt;\n   vip()\n\n\n\n\n\n\nTest model performance through C-index:\n\n# Test model performance -------------------------------\ntest_result &lt;- final_fit |&gt; test_wrnet(df_test)\n# Overall and event-specific C-indices\ntest_result$concordance\n#&gt; # A tibble: 3 Ã— 2\n#&gt;   component concordance\n#&gt;   &lt;chr&gt;           &lt;dbl&gt;\n#&gt; 1 death           0.724\n#&gt; 2 nonfatal        0.607\n#&gt; 3 overall         0.664\n\n\n\nNote\nBoth cv_wrnet and wrnet functions accept additional arguments and pass them to the underlying glmnet::glmnet(). For example, use alpha = 0.5 in both to specify a half-lasso and half-ridge penalty, instead of the default alpha = 1 (lasso).\n\n\nReferences"
  }
]